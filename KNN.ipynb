{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest Neighbour algorithm\n",
    "## Imports\n",
    "Relevant imports for this algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "import imblearn\n",
    "from imblearn.combine import SMOTETomek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "Load dataset and separate X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the dataset\n",
    "def loadData():\n",
    "    df = pd.read_csv('telecom_churn.csv')\n",
    "    # Remove churn from X and put it in y\n",
    "    df.drop([\"MonthlyCharge\", \"DataUsage\"], axis=1)\n",
    "    X = df.drop(\"Churn\",axis=1)\n",
    "    y = df[\"Churn\"]\n",
    "    return df,X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split in data set, validation set and test set\n",
    "Splits the data into three parts; Train, validation and test set. Training set for training, validation to fine tune hyperparameters and test set to train the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData(X, y, test_size, random_state):\n",
    "    # Split dataset into train, validation and test data\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=y) \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=test_size, random_state=random_state, stratify=y_train_val) \n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resample of training data set\n",
    "First resample method. Resample method from SKlearn. Used to try combined method with both oversampling and undersampling. Did not give optimal results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resample method 1 - resample from SKlearn\n",
    "def resample_dataset(X_train, y_train, random_state):\n",
    "    df_train = X_train.copy()\n",
    "    df_train[\"Churn\"]=y_train\n",
    "\n",
    "    df_majority = df_train[df_train.Churn == 0]\n",
    "    df_minority = df_train[df_train.Churn == 1]\n",
    "\n",
    "    df_min_up = resample(df_minority, replace=True, n_samples=750, random_state=random_state) #750\n",
    "    df_maj_up = resample(df_majority, replace=False, n_samples=1500, random_state=random_state)\n",
    "\n",
    "    df_up = pd.concat([df_min_up, df_maj_up])\n",
    "    \n",
    "    print(\"Counts after resample\\n\", df_up.Churn.value_counts())\n",
    "    X_train = df_up.drop(\"Churn\",axis=1)\n",
    "    y_train = df_up[\"Churn\"] \n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we tried SMOTETomek. Resamples as explained in the method paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resample method 2 - SMOTETomek from ImbLearn\n",
    "def resample_SMOTE(X_train, y_train, random_state):\n",
    "    smote = SMOTETomek(sampling_strategy=1.0, random_state=random_state)\n",
    "    X_smote, y_smote = smote.fit_sample(X, y)\n",
    "\n",
    "    return X_smote, y_smote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale data set\n",
    "Scales the data to more optimal values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data into \n",
    "# Inspiration from examples from\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "# https://stackabuse.com/k-nearest-neighbors-algorithm-in-python-and-scikit-learn/\n",
    "def scale(X_train, X_val, X_test):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train, X_val, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier\n",
    "Trains the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train classifier\n",
    "def train_classifier(X_train, y_train, neighbors, metric):\n",
    "    classifier = KNeighborsClassifier(n_neighbors=neighbors, metric=metric) #24\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best metric\n",
    "Train a classifier with each distance matric and find out which metric works best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_metric(metrics, X_train, y_train, X_val, y_val):\n",
    "    metric_best = None\n",
    "    best_f1_score = 0 \n",
    "    for metric in metrics:\n",
    "        classifier = train_classifier(X_train, y_train, 37, metric)\n",
    "        y_pred = classifier.predict(X_val)\n",
    "        score = round(f1_score(y_val, y_pred), 4)\n",
    "        recall = round(recall_score(y_val, y_pred), 4)\n",
    "        precision = round(precision_score(y_val, y_pred), 4)\n",
    "        accuracy = round(accuracy_score(y_val, y_pred), 4)\n",
    "        print(\"Metric: \", metric, \" F1 score: \", score, \" Recall: \", recall, \n",
    "              \" Precision: \", precision, \" Accuracy: \", accuracy)\n",
    "        if score > best_f1_score:\n",
    "            metric_best = metric\n",
    "            best_f1_score = score\n",
    "        \n",
    "    print(\"Best metric: \", metric_best)\n",
    "    return metric_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs\n",
    "Visualizations as classification report, confusion matrix, and error graph\n",
    "\n",
    "### Classification report and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_classification_report(y, y_pred, fig_name):\n",
    "    cfm = confusion_matrix(y, y_pred)\n",
    "\n",
    "    target_names = ['Not Churn', 'Churn']\n",
    "    #Classification_report\n",
    "    print(\"\\nClassification report\\n\")\n",
    "    print(classification_report(y, y_pred, target_names=target_names))\n",
    "\n",
    "    # The code below is taken from\n",
    "    # https://medium.com/@dtuk81/confusion-matrix-visualization-fc31e3f30fea\n",
    "    # with minor changes\n",
    "    group_names = [\"True Negative\",\"False Positive\",\"False Negative\",\"True Positive\"]\n",
    "    group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                    cfm.flatten()]\n",
    "    group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                         cfm.flatten()/np.sum(cfm)]\n",
    "    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "              zip(group_names,group_counts,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "\n",
    "    sns_plot = sns.heatmap(cfm, annot=labels, fmt=\"\", cmap='Blues')\n",
    "    sns_plot.set_title(\"Confusion matrix\")\n",
    "    sns_plot.set_xlabel(\"Predicted\")\n",
    "    sns_plot.set_ylabel(\"Actual\")\n",
    "    sns_plot.figure.savefig('images/knn/' + fig_name + '.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_graph(X_train, y_train, X_val, y_val, metric):\n",
    "    error = []\n",
    "\n",
    "    # Calculating error for K values between 1 and 40\n",
    "    # Code taken from\n",
    "    # https://stackabuse.com/k-nearest-neighbors-algorithm-in-python-and-scikit-learn/\n",
    "    # with minor changes\n",
    "    for i in range(1, 40):\n",
    "        classifier = KNeighborsClassifier(n_neighbors=i, metric=metric)\n",
    "        classifier.fit(X_train, y_train)\n",
    "        pred_i = classifier.predict(X_val)\n",
    "        error.append(np.mean(pred_i != y_val))\n",
    "    neighbours = error.index(min(error))+1\n",
    "    print(\"Neighbours with minimum error: \", neighbours)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(range(1, 40), error, color='red', linestyle='dashed', marker='o',\n",
    "             markerfacecolor='blue', markersize=10)\n",
    "    plt.title('Error Rate K Value')\n",
    "    plt.xlabel('K Value')\n",
    "    plt.ylabel('Mean Error')\n",
    "    plt.savefig('images/knn/error_graph_knn.png')\n",
    "    return neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train dataset\n",
    "Loads and splits the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random_state and test_split\n",
    "random_state = 1\n",
    "test_split = 0.2\n",
    "\n",
    "#Load data set\n",
    "df, X, y = loadData()\n",
    "\n",
    "#Split data set\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = splitData(X, y, test_split, random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test before preprocessing and fine tuning hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train classifier to find best K value\n",
    "classifier = train_classifier(X_train, y_train, 5, \"minkowski\") \n",
    "#5 is default value for n_neighbours, minkowski is default metric\n",
    "y_pred = classifier.predict(X_test)\n",
    "show_classification_report(y_test, y_pred, \"confusion_matrix_knn_original\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resamples and scales the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resample data set\n",
    "# X_train, y_train = resample_dataset(X_train, y_train, random_state)\n",
    "X_train, y_train = resample_SMOTE(X_train, y_train, random_state)\n",
    "\n",
    "#Scale data set\n",
    "X_train, X_val, X_test = scale(X_train, X_val, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter fine tuning\n",
    "Finds the best metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# The metrics we are going to test with to find the best metric\n",
    "metrics = [\"euclidean\", \"manhattan\", \"minkowski\", \"chebyshev\"]\n",
    "\n",
    "#Best metric\n",
    "metric = best_metric(metrics, X_train, y_train, X_val, y_val) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finds the number of neighbours with minimum error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find n: number of neighbours with minimum error rate\n",
    "n = error_graph(X_train, y_train, X_val, y_val, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train classifier \n",
    "classifier = train_classifier(X_train, y_train, n, metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checks out with the validation value on how well it does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "y_pred_val = classifier.predict(X_val)\n",
    "show_classification_report(y_val, y_pred_val, \"confusion_matrix_knn_validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "show_classification_report(y_test, y_pred, \"confusion_matrix_knn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
