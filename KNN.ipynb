{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Nearest Neighbour algorithm\n",
    "## Imports\n",
    "Relevant imports for this algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackabuse.com/k-nearest-neighbors-algorithm-in-python-and-scikit-learn/\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "import imblearn\n",
    "from imblearn.combine import SMOTETomek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData():\n",
    "    df = pd.read_csv('telecom_churn.csv')\n",
    "    # Remove churn from X and put it in y\n",
    "    X = df.drop(\"Churn\",axis=1)\n",
    "    y = df[\"Churn\"]\n",
    "    return df,X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split in data set, validation set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData(X, y, test_size, random_state):\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=y) #, stratify=y, 1\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=test_size, random_state=random_state, stratify=y_train_val) #, stratify=y_train_val, 1\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resample of training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_dataset(X_train, y_train, random_state):\n",
    "    df_train = X_train.copy()\n",
    "    df_train[\"Churn\"]=y_train\n",
    "\n",
    "    df_majority = df_train[df_train.Churn == 0]\n",
    "    df_minority = df_train[df_train.Churn == 1]\n",
    "\n",
    "    df_min_up = resample(df_minority, replace=True, n_samples=750, random_state=random_state) #750\n",
    "    #print(df_minority.shape, df_majority.shape, df_min_up.shape)\n",
    "\n",
    "    df_maj_up = resample(df_majority, replace=False, n_samples=750, random_state=random_state)\n",
    "    #print(df_minority.shape, df_majority.shape, df_maj_up.shape)\n",
    "\n",
    "    df_up = pd.concat([df_min_up, df_maj_up])\n",
    "    \n",
    "    print(\"Counts after resample\\n\", df_up.Churn.value_counts())\n",
    "    X_train = df_up.drop(\"Churn\",axis=1)\n",
    "    y_train = df_up[\"Churn\"] \n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_SMOTE(X_train, y_train, random_state):\n",
    "    smt = SMOTETomek(sampling_strategy=0.25)\n",
    "    X_smt, y_smt = smt.fit_sample(X, y)\n",
    "\n",
    "    plot_2d_space(X_smt, y_smt, 'SMOTE + Tomek links')\n",
    "    return X_smt, y_smt\n",
    "\n",
    "# smt = SMOTETomek(random_state=42)\n",
    "# X_res, y_res = smt.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(X_train, X_val, X_test):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train, X_val, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(X_train, y_train, neighbors, metric):\n",
    "    classifier = KNeighborsClassifier(n_neighbors=neighbors, metric=metric) #24\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(classifier, X_val, y_val, scoring):\n",
    "    classifier_cross_val = cross_val_score(classifier, X_val, y_val, scoring=scoring)\n",
    "    print(\"\\nCross_validation score: {:.2f}\".format(classifier_cross_val.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best metric\n",
    "train a classifier with each distance matric and find out which metric works best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_metric(metrics, X_train, y_train, X_val, y_val): #accuracy_score, precision_score, recall_score\n",
    "    metric_best = None\n",
    "    best_f1_score = 0 \n",
    "    for metric in metrics:\n",
    "        clf = train_classifier(X_train, y_train, 37, metric)\n",
    "        y_pred = clf.predict(X_val)\n",
    "        score = round(f1_score(y_val, y_pred), 4)\n",
    "        recall = round(recall_score(y_val, y_pred), 4)\n",
    "        precision = round(precision_score(y_val, y_pred), 4)\n",
    "        accuracy = round(accuracy_score(y_val, y_pred), 4)\n",
    "        print(\"Metric: \", metric, \" F1 score: \", score, \" Recall: \", recall, \n",
    "              \" Precision: \", precision, \" Accuracy: \", accuracy)\n",
    "        if score > best_f1_score:\n",
    "            metric_best = metric\n",
    "            best_f1_score = score\n",
    "        \n",
    "    print(\"Best metric: \", metric_best)\n",
    "    return metric_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs\n",
    "Visualizations as classification report, confusion matrix, and error graph\n",
    "\n",
    "### Classification report and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_classification_report(y, y_pred):\n",
    "    cf_matrix = confusion_matrix(y, y_pred)\n",
    "#     print(cf_matrix)\n",
    "    target_names = ['Not Churn', 'Churn']\n",
    "    #Classification_report\n",
    "    print(\"\\nClassification report\\n\")\n",
    "    print(classification_report(y, y_pred, target_names=target_names))\n",
    "\n",
    "    #https://medium.com/@dtuk81/confusion-matrix-visualization-fc31e3f30fea\n",
    "    group_names = [\"True Negative\",\"False Positive\",\"False Negative\",\"True Positive\"]\n",
    "    group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                    cf_matrix.flatten()]\n",
    "    group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                         cf_matrix.flatten()/np.sum(cf_matrix)]\n",
    "    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "              zip(group_names,group_counts,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "    print(\"\\nConfusion matrix\")\n",
    "    sns_plot = sns.heatmap(cf_matrix, annot=labels, fmt=\"\", cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_graph(X_train, y_train, X_val, y_val, metric):\n",
    "    error = []\n",
    "\n",
    "    # Calculating error for K values between 1 and 40\n",
    "    for i in range(1, 40):\n",
    "        knn = KNeighborsClassifier(n_neighbors=i, metric=metric)\n",
    "        knn.fit(X_train, y_train)\n",
    "        pred_i = knn.predict(X_val)\n",
    "        error.append(np.mean(pred_i != y_val))\n",
    "    neighbours = error.index(min(error))+1\n",
    "    print(\"Neighbours with minimum error: \", neighbours)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(range(1, 40), error, color='red', linestyle='dashed', marker='o',\n",
    "             markerfacecolor='blue', markersize=10)\n",
    "    plt.title('Error Rate K Value')\n",
    "    plt.xlabel('K Value')\n",
    "    plt.ylabel('Mean Error')\n",
    "    return neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_graph(X_val, y_val, metric):\n",
    "    k_range = range(1, 31)\n",
    "    k_f1_scores = []\n",
    "    k_recall_scores = []\n",
    "    # use iteration to caclulator different k in models, then return the average accuracy based on the cross validation\n",
    "    for k in k_range:\n",
    "        knn = KNeighborsClassifier(n_neighbors=k, metric=metric)\n",
    "        f1_scores = cross_val_score(knn, X_val, y_val, cv=5, scoring='f1')\n",
    "        recall_scores = cross_val_score(knn, X_val, y_val, cv=5, scoring='recall')\n",
    "        k_f1_scores.append(f1_scores.mean())# plot to see clealy\n",
    "        k_recall_scores.append(recall_scores.mean())# plot to see clealy\n",
    "\n",
    "    plt.plot(k_range, k_f1_scores, marker='o')\n",
    "    plt.plot(k_range, k_recall_scores, marker='o')\n",
    "    plt.xlabel('Value of K for KNN')\n",
    "    plt.ylabel('Cross-Validated Accuracy')\n",
    "    plt.show()\n",
    "\n",
    "    k_neighbor = k_f1_scores.index(max(k_f1_scores))+1\n",
    "    print(k_neighbor)\n",
    "    print(k_recall_scores.index(max(k_recall_scores))+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "from matplotlib.legend_handler import HandlerLine2D\n",
    "\n",
    "def auc_score(x_train, y_train, x_test, y_test):\n",
    "    neighbors = list(range(1,30))\n",
    "    train_results = []\n",
    "    test_results = []\n",
    "    for n in neighbors:\n",
    "        model = KNeighborsClassifier(n_neighbors=n)\n",
    "        model.fit(x_train, y_train)\n",
    "        train_pred = model.predict(x_train)\n",
    "        false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, train_pred)\n",
    "        roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "        train_results.append(roc_auc)\n",
    "        y_pred = model.predict(x_test)\n",
    "        false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n",
    "        roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "        test_results.append(roc_auc)\n",
    "\n",
    "\n",
    "    line1, = plt.plot(neighbors, train_results, \"b\", label=\"Train AUC\")\n",
    "    line2, = plt.plot(neighbors, test_results, \"r\", label=\"Test AUC\")\n",
    "    plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
    "    plt.ylabel(\"AUC score\")\n",
    "    plt.xlabel(\"n_neighbors\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "def plot_2d_space(X, y, label='Classes'): \n",
    "    X = pca.fit_transform(X)\n",
    "    colors = ['#1F77B4', '#FF7F0E']\n",
    "    markers = ['o', 's']\n",
    "    for l, c, m in zip(np.unique(y), colors, markers):\n",
    "        plt.scatter(\n",
    "            X[y==l, 0],\n",
    "            X[y==l, 1],\n",
    "            c=c, label=l, marker=m\n",
    "        )\n",
    "    plt.title(label)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random_state and test_split\n",
    "random_state = 1\n",
    "test_split = 0.2\n",
    "\n",
    "#Load data set\n",
    "df, X, y = loadData()\n",
    "plot_2d_space(X, y)\n",
    "#Split data set\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = splitData(X, y, test_split, random_state)\n",
    "plot_2d_space(X_train, y_train)\n",
    "#Resample data set\n",
    "# X_train, y_train = resample_dataset(X_train, y_train, random_state)\n",
    "X_train, y_train = resample_SMOTE(X_train, y_train, random_state)\n",
    "plot_2d_space(X_train, y_train)\n",
    "#Scale data set\n",
    "X_train, X_val, X_test = scale(X_train, X_val, X_test)\n",
    "plot_2d_space(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# The metrics we are going to test with to find the best metric\n",
    "metrics = [\"euclidean\", \"manhattan\", \"minkowski\", \"chebyshev\"]\n",
    "#euclidean, manhattan, chebyshev, minkowski, wminkowski, seuclidean, mahalanobis\n",
    "\n",
    "#Best metric\n",
    "metric = best_metric(metrics, X_train, y_train, X_val, y_val) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find n: number of neighbours with minimum error rate\n",
    "n = error_graph(X_train, y_train, X_val, y_val, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_score(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train classifier to find best K value\n",
    "clf = train_classifier(X_train, y_train, n, metric)\n",
    "#Cross validation score\n",
    "cross_validation(clf, X_val, y_val, \"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "y_pred_val = clf.predict(X_val)\n",
    "show_classification_report(y_val, y_pred_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with test values\n",
    "clf = train_classifier(X_train, y_train, 5, metric)\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "y_pred = clf.predict(X_test)\n",
    "show_classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
